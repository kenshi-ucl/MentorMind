"""Agent orchestrator service for managing AI agents."""
import json
import os
from pathlib import Path
from typing import Optional
from app.models.agent_prompt import AgentPrompt


class AgentOrchestrator:
    """Service for orchestrating AI agent operations."""
    
    def __init__(self, prompts_dir: Optional[str] = None):
        """
        Initialize the agent orchestrator.
        
        Args:
            prompts_dir: Path to the directory containing agent prompt JSON files.
                        Defaults to .kiro/prompts relative to project root.
        """
        if prompts_dir is None:
            # Default to .kiro/prompts relative to project root
            project_root = Path(__file__).parent.parent.parent.parent
            prompts_dir = str(project_root / ".kiro" / "prompts")
        
        self._prompts_dir = prompts_dir
        self._agents: dict[str, AgentPrompt] = {}
        self._loaded = False
    
    def _load_agents(self) -> None:
        """Load all agent configurations from JSON files."""
        if self._loaded:
            return
        
        agent_names = ["TutorAgent", "QuizAgent", "ContentAgent"]
        
        for agent_name in agent_names:
            agent = self._load_agent(agent_name)
            if agent:
                self._agents[agent_name] = agent
        
        self._loaded = True
    
    def _load_agent(self, agent_name: str) -> Optional[AgentPrompt]:
        """
        Load a single agent configuration from its JSON file.
        
        Args:
            agent_name: Name of the agent (e.g., "TutorAgent")
            
        Returns:
            AgentPrompt if loaded successfully, None otherwise.
        """
        file_path = os.path.join(self._prompts_dir, f"{agent_name}.json")
        
        if not os.path.exists(file_path):
            return None
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return AgentPrompt.from_dict(data)
        except (json.JSONDecodeError, IOError):
            return None
    
    def get_agent(self, agent_name: str) -> Optional[AgentPrompt]:
        """
        Get an agent's prompt configuration.
        
        Args:
            agent_name: Name of the agent to retrieve.
            
        Returns:
            AgentPrompt if found, None otherwise.
        """
        self._load_agents()
        return self._agents.get(agent_name)
    
    def get_all_agents(self) -> dict[str, AgentPrompt]:
        """
        Get all loaded agent configurations.
        
        Returns:
            Dictionary mapping agent names to their configurations.
        """
        self._load_agents()
        return self._agents.copy()
    
    def process_chat(self, message: str, context: Optional[list[str]] = None) -> str:
        """
        Process a chat message through the TutorAgent.
        
        Args:
            message: The user's message/question.
            context: Optional list of content context strings.
            
        Returns:
            The agent's response string.
        """
        self._load_agents()
        
        tutor = self._agents.get("TutorAgent")
        if not tutor:
            return "TutorAgent is not available."
        
        # Build the prompt with context
        prompt_parts = [tutor.system_prompt]
        
        if context:
            prompt_parts.append("\n\nContent Context:")
            for ctx in context:
                prompt_parts.append(f"- {ctx}")
        
        prompt_parts.append(f"\n\nUser Question: {message}")
        
        # In a real implementation, this would call an LLM API
        # For now, return a placeholder response
        full_prompt = "\n".join(prompt_parts)
        
        # Placeholder response - would be replaced with actual LLM call
        return f"[TutorAgent Response] I received your question: '{message}'. " \
               f"{'I have context from your uploaded content. ' if context else ''}" \
               f"This is a placeholder response that would be generated by an LLM."
    
    def generate_quiz(self, topic: Optional[str] = None, 
                      content: Optional[str] = None,
                      question_count: int = 5) -> list[dict]:
        """
        Generate a quiz using the QuizAgent.
        
        Args:
            topic: Optional topic for the quiz.
            content: Optional content summary to base questions on.
            question_count: Number of questions to generate.
            
        Returns:
            List of quiz question dictionaries.
        """
        self._load_agents()
        
        quiz_agent = self._agents.get("QuizAgent")
        if not quiz_agent:
            return []
        
        # Build the prompt
        prompt_parts = [quiz_agent.system_prompt]
        
        if topic:
            prompt_parts.append(f"\n\nTopic: {topic}")
        
        if content:
            prompt_parts.append(f"\n\nContent Summary: {content}")
        
        prompt_parts.append(f"\n\nGenerate {question_count} questions.")
        
        # In a real implementation, this would call an LLM API
        # For now, return placeholder questions
        placeholder_questions = []
        for i in range(question_count):
            placeholder_questions.append({
                "id": f"q{i+1}",
                "question": f"Placeholder question {i+1} about {topic or 'the content'}?",
                "options": ["Option A", "Option B", "Option C", "Option D"],
                "correct_index": 0,
                "explanation": "This is a placeholder explanation."
            })
        
        return placeholder_questions
    
    def process_content(self, content_text: str, 
                        content_type: str = "text") -> dict:
        """
        Process content through the ContentAgent to extract key information.
        
        Args:
            content_text: The raw text content to process.
            content_type: Type of content ('video', 'pdf', 'text').
            
        Returns:
            Dictionary containing extracted information.
        """
        self._load_agents()
        
        content_agent = self._agents.get("ContentAgent")
        if not content_agent:
            return {"error": "ContentAgent is not available."}
        
        # Build the prompt
        prompt_parts = [content_agent.system_prompt]
        prompt_parts.append(f"\n\nContent Type: {content_type}")
        prompt_parts.append(f"\n\nContent:\n{content_text}")
        
        # In a real implementation, this would call an LLM API
        # For now, return placeholder extraction
        return {
            "title": "Extracted Content",
            "summary": f"Summary of the {content_type} content.",
            "key_points": [
                "Key point 1 extracted from content",
                "Key point 2 extracted from content",
                "Key point 3 extracted from content"
            ],
            "concepts": [
                {
                    "term": "Concept 1",
                    "definition": "Definition of concept 1"
                }
            ],
            "topics": ["Topic 1", "Topic 2"]
        }
    
    def reload_agents(self) -> None:
        """Force reload of all agent configurations."""
        self._agents.clear()
        self._loaded = False
        self._load_agents()


# Global agent orchestrator instance
agent_orchestrator = AgentOrchestrator()
